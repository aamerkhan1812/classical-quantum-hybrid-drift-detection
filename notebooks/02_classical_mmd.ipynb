{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3b65d-4497-4f9d-b570-8f34559acf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Classical drift detection on 150 windows ===\n",
    "import os, time, math\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import ks_2samp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "BASE_CSV = \"qml_drift/data/creditcard.csv\"\n",
    "OUT_DIR = \"results/creditcard/classical_150_windows\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"classical_150_windows.csv\")\n",
    "\n",
    "N_REF = 4000\n",
    "N_TEST = 1000\n",
    "STRIDE = 500\n",
    "WINDOW_COUNT = 150            \n",
    "WINDOW_SELECTION = \"first\"    \n",
    "# Performance / statistical tradeoffs\n",
    "N_JOBS = max(1, (os.cpu_count() or 2) - 1)\n",
    "B_PERM = 100                  # permutations per window (reduce to 50 if too slow)\n",
    "SAMPLE_FOR_GAMMA = 2000\n",
    "SUBSAMPLE_FOR_KERNEL = 1000   # set to integer to reduce kernel O(n^2); None to use full R/N\n",
    "SEED = 42\n",
    "# ---------------------------------------\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---------- helper fns  ----------\n",
    "def median_heuristic_gamma_sample(X, max_sample=SAMPLE_FOR_GAMMA):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n = X.shape[0]\n",
    "    if n > max_sample:\n",
    "        idx = np.random.choice(n, max_sample, replace=False)\n",
    "        sub = X[idx]\n",
    "    else:\n",
    "        sub = X\n",
    "    if sub.shape[0] < 2:\n",
    "        return 1.0\n",
    "    dists = pdist(sub, metric='euclidean')\n",
    "    med = np.median(dists) if len(dists) > 0 else 1.0\n",
    "    if med <= 0:\n",
    "        return 1.0\n",
    "    return 1.0 / (2.0 * (med**2))\n",
    "\n",
    "def rbf_kernel(X, Y, gamma):\n",
    "    X = np.asarray(X, dtype=float); Y = np.asarray(Y, dtype=float)\n",
    "    XX = np.sum(X * X, axis=1)[:, None]\n",
    "    YY = np.sum(Y * Y, axis=1)[None, :]\n",
    "    D2 = XX + YY - 2.0 * X.dot(Y.T)\n",
    "    K = np.exp(-gamma * D2)\n",
    "    return K\n",
    "\n",
    "def mmd_unbiased_from_kernel(Kxx, Kyy, Kxy):\n",
    "    n = Kxx.shape[0]; m = Kyy.shape[0]\n",
    "    if n < 2 or m < 2:\n",
    "        return 0.0\n",
    "    sum_xx = (np.sum(Kxx) - np.trace(Kxx)) / (n * (n - 1) + 1e-12)\n",
    "    sum_yy = (np.sum(Kyy) - np.trace(Kyy)) / (m * (m - 1) + 1e-12)\n",
    "    sum_xy = np.sum(Kxy) / (n * m + 1e-12)\n",
    "    return float(sum_xx + sum_yy - 2 * sum_xy)\n",
    "\n",
    "def mmd_stat_from_data(X, Y, gamma=None, sample_for_gamma=SAMPLE_FOR_GAMMA):\n",
    "    if gamma is None:\n",
    "        gamma = median_heuristic_gamma_sample(np.vstack([X, Y]), max_sample=sample_for_gamma)\n",
    "    Kxx = rbf_kernel(X, X, gamma)\n",
    "    Kyy = rbf_kernel(Y, Y, gamma)\n",
    "    Kxy = rbf_kernel(X, Y, gamma)\n",
    "    return mmd_unbiased_from_kernel(Kxx, Kyy, Kxy), float(gamma)\n",
    "\n",
    "def permutation_p_value(X, Y, stat_func, B=B_PERM, seed=SEED):\n",
    "    np.random.seed(seed)\n",
    "    Z = np.vstack([X, Y])\n",
    "    n = X.shape[0]; m = Y.shape[0]\n",
    "    if n == 0 or m == 0:\n",
    "        return 0.0, 1.0, np.array([])\n",
    "    obs = stat_func(X, Y)\n",
    "    cnt = 0\n",
    "    stats = []\n",
    "    for b in range(B):\n",
    "        idx = np.random.permutation(n + m)\n",
    "        Xp = Z[idx[:n]]\n",
    "        Yp = Z[idx[n:]]\n",
    "        s = stat_func(Xp, Yp)\n",
    "        stats.append(float(s))\n",
    "        if s >= obs:\n",
    "            cnt += 1\n",
    "    pval = (cnt + 1) / (B + 1)\n",
    "    return float(obs), float(pval), np.array(stats, dtype=float)\n",
    "\n",
    "def psi_score(a, b, n_bins=10):\n",
    "    a = np.asarray(a); b = np.asarray(b)\n",
    "    try:\n",
    "        bins = np.quantile(a, q=np.linspace(0, 1, n_bins + 1))\n",
    "    except Exception:\n",
    "        bins = np.linspace(min(a.min(), b.min()), max(a.max(), b.max()), n_bins + 1)\n",
    "    bins = np.unique(bins)\n",
    "    if len(bins) <= 1:\n",
    "        return 0.0\n",
    "    a_counts, _ = np.histogram(a, bins=bins)\n",
    "    b_counts, _ = np.histogram(b, bins=bins)\n",
    "    pa = a_counts / (np.sum(a_counts) + 1e-12)\n",
    "    pb = b_counts / (np.sum(b_counts) + 1e-12)\n",
    "    pa = np.where(pa == 0, 1e-8, pa)\n",
    "    pb = np.where(pb == 0, 1e-8, pb)\n",
    "    return float(np.sum((pa - pb) * np.log(pa / pb)))\n",
    "\n",
    "# ---------- sliding windows helper ----------\n",
    "def sliding_windows_indices(n_total, n_ref=N_REF, n_test=N_TEST, stride=STRIDE):\n",
    "    indices = []\n",
    "    max_start = n_total - (n_ref + n_test)\n",
    "    start = 0\n",
    "    win_id = 0\n",
    "    while start <= max_start:\n",
    "        indices.append((win_id, start, start + n_ref, start + n_ref + n_test))\n",
    "        start += stride\n",
    "        win_id += 1\n",
    "    return indices\n",
    "\n",
    "# ---------- worker that uses preprocess_window (or fallback to user-defined process_window) ----------\n",
    "def _worker_from_preprocess(window_tuple):\n",
    "    win_id, sref, stest, etest = window_tuple\n",
    "    try:\n",
    "        R_df = df_all.iloc[sref:stest].reset_index(drop=True)\n",
    "        N_df = df_all.iloc[stest:etest].reset_index(drop=True)\n",
    "        pre = preprocess_window(R_df, N_df)   # assumes preprocess_window exists\n",
    "        R_cls = np.asarray(pre['R_cls'], dtype=float)\n",
    "        N_cls = np.asarray(pre['N_cls'], dtype=float)\n",
    "        R_scaled = pre['R_scaled_df']; N_scaled = pre['N_scaled_df']\n",
    "        # optional subsample for kernel speed\n",
    "        if SUBSAMPLE_FOR_KERNEL is not None:\n",
    "            nx = R_cls.shape[0]; ny = N_cls.shape[0]\n",
    "            ix = np.random.choice(np.arange(nx), size=min(SUBSAMPLE_FOR_KERNEL, nx), replace=False)\n",
    "            iy = np.random.choice(np.arange(ny), size=min(SUBSAMPLE_FOR_KERNEL, ny), replace=False)\n",
    "            Rm = R_cls[ix]; Nm = N_cls[iy]\n",
    "        else:\n",
    "            Rm = R_cls; Nm = N_cls\n",
    "        mmd_val, gamma_used = mmd_stat_from_data(Rm, Nm, gamma=None)\n",
    "        obs, pval, perm_stats = permutation_p_value(Rm, Nm, lambda a,b: mmd_stat_from_data(a,b,gamma_used)[0],\n",
    "                                                   B=B_PERM, seed=(SEED + int(win_id)))\n",
    "        ks_list = []; psi_list = []\n",
    "        for c in pre.get('feat_cols', []):\n",
    "            try:\n",
    "                ks = ks_2samp(R_scaled[c].values, N_scaled[c].values)\n",
    "                ks_list.append((c, float(ks.statistic), float(ks.pvalue)))\n",
    "            except Exception:\n",
    "                ks_list.append((c, 0.0, 1.0))\n",
    "            try:\n",
    "                psi_v = psi_score(R_scaled[c].values, N_scaled[c].values)\n",
    "                psi_list.append((c, float(psi_v)))\n",
    "            except Exception:\n",
    "                psi_list.append((c, 0.0))\n",
    "        ks_sorted = sorted(ks_list, key=lambda x: -x[1])[:3]\n",
    "        psi_sorted = sorted(psi_list, key=lambda x: -x[1])[:3]\n",
    "        return {\n",
    "            \"window_id\": int(win_id),\n",
    "            \"start_ref\": int(sref),\n",
    "            \"start_test\": int(stest),\n",
    "            \"end_test\": int(etest),\n",
    "            \"mmd_stat\": float(mmd_val),\n",
    "            \"gamma\": float(gamma_used),\n",
    "            \"p_cls\": float(pval),\n",
    "            \"perm_mean\": float(np.mean(perm_stats)) if perm_stats.size>0 else float(\"nan\"),\n",
    "            \"perm_std\": float(np.std(perm_stats)) if perm_stats.size>0 else float(\"nan\"),\n",
    "            \"top_ks\": \";\".join([f\"{k}:{stat:.4f}:{pv:.1e}\" for (k,stat,pv) in ks_sorted]),\n",
    "            \"top_psi\": \";\".join([f\"{k}:{v:.4f}\" for (k,v) in psi_sorted])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"window_id\": int(win_id), \"start_ref\": int(sref), \"start_test\": int(stest),\n",
    "                \"end_test\": int(etest), \"error\": str(e)}\n",
    "\n",
    "# ---------- choose worker: if process_window exists, use it; else use local preprocess worker ----------\n",
    "_worker = None\n",
    "try:\n",
    "    # prefer user-defined process_window if available\n",
    "    if 'process_window' in globals() and callable(process_window):\n",
    "        _worker = process_window\n",
    "        print(\"Using existing user-defined process_window()\")\n",
    "    else:\n",
    "        raise NameError\n",
    "except Exception:\n",
    "    _worker = _worker_from_preprocess\n",
    "    print(\"Using internal worker that calls preprocess_window()\")\n",
    "\n",
    "# ---------- load data and pick windows ----------\n",
    "print(\"Loading dataset ...\")\n",
    "df_all = pd.read_csv(BASE_CSV)\n",
    "n_total = len(df_all)\n",
    "all_indices = sliding_windows_indices(n_total, N_REF, N_TEST, STRIDE)\n",
    "n_all_windows = len(all_indices)\n",
    "if WINDOW_COUNT <= 0 or WINDOW_COUNT > n_all_windows:\n",
    "    WINDOW_COUNT = n_all_windows\n",
    "\n",
    "# select windows\n",
    "if WINDOW_SELECTION == \"first\":\n",
    "    sel_indices = all_indices[:WINDOW_COUNT]\n",
    "elif WINDOW_SELECTION == \"even\":\n",
    "    # sample WINDOW_COUNT indices evenly across entire range\n",
    "    idxs = np.linspace(0, n_all_windows-1, WINDOW_COUNT, dtype=int)\n",
    "    sel_indices = [all_indices[i] for i in idxs]\n",
    "else:\n",
    "    sel_indices = all_indices[:WINDOW_COUNT]\n",
    "\n",
    "print(f\"Total rows: {n_total}  → All windows: {n_all_windows}  → Selected windows: {len(sel_indices)}\")\n",
    "print(f\"Running with N_jobs={N_JOBS}, B_permutations={B_PERM}, SUBSAMPLE_FOR_KERNEL={SUBSAMPLE_FOR_KERNEL}\")\n",
    "\n",
    "# ---------- run in parallel ----------\n",
    "t0 = time.time()\n",
    "results = Parallel(n_jobs=N_JOBS, prefer=\"threads\")(\n",
    "    delayed(_worker)(idx_tuple) for idx_tuple in tqdm(sel_indices, desc=\"Processing windows\")\n",
    ")\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nDone classical stage. Time taken: {elapsed/60:.2f} minutes (elapsed {elapsed:.1f}s)\")\n",
    "\n",
    "# ---------- save ----------\n",
    "df_out = pd.DataFrame(results)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved classical results to:\", OUT_CSV)\n",
    "\n",
    "# diagnostics\n",
    "n_err = df_out['error'].notna().sum() if 'error' in df_out.columns else 0\n",
    "print(f\"Windows processed: {len(df_out)}; windows with error: {n_err}\")\n",
    "if n_err > 0:\n",
    "    print(\"Example errors (first 5):\")\n",
    "    print(df_out[df_out['error'].notna()].head(5)[['window_id','error']])\n",
    "else:\n",
    "    print(\"No errors detected in result rows.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
