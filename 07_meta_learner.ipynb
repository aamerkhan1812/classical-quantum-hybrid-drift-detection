{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75692d94-4e98-49c9-9a4d-ba32355d7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP-Constrained Meta-Learner \n",
    "import os, pickle, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, average_precision_score\n",
    "\n",
    "# --- CONFIG ---\n",
    "CSV_PATH = \"results/creditcard/dataset_with_drift/dataset_with_drift.csv\"\n",
    "OUT_DIR = \"results/creditcard/results/full_run_fp_constrained\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "N_SPLITS = 5\n",
    "SEED = 42\n",
    "FP_RATE_CAP = 0.05   \n",
    "# ----------------\n",
    "\n",
    "# 1) load\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded:\", CSV_PATH, \"shape:\", df.shape)\n",
    "\n",
    "# 2) build features \n",
    "def safe_neglog(p):\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    p = np.clip(p, 1e-12, 1.0)\n",
    "    return -np.log(p)\n",
    "\n",
    "feat_candidates = []\n",
    "\n",
    "for col in [\"S_hybrid_robust\", \"S_roll_mean\", \"S_roll_std\", \"T_rolling\", \"w_q_gated\", \"kernel_var\", \"p_cls\", \"p_q\"]:\n",
    "    if col in df.columns:\n",
    "        feat_candidates.append(col)\n",
    "\n",
    "if \"p_cls\" in df.columns:\n",
    "    df[\"neglog_p_cls\"] = safe_neglog(df[\"p_cls\"])\n",
    "    feat_candidates.append(\"neglog_p_cls\")\n",
    "if \"p_q\" in df.columns:\n",
    "    df[\"neglog_p_q\"] = safe_neglog(df[\"p_q\"])\n",
    "    feat_candidates.append(\"neglog_p_q\")\n",
    "\n",
    "features = []\n",
    "for f in feat_candidates:\n",
    "    if f in df.columns and f not in features:\n",
    "        features.append(f)\n",
    "\n",
    "if len(features) == 0:\n",
    "    raise ValueError(\"No features available. Need at least p_cls or S_hybrid_robust in CSV.\")\n",
    "\n",
    "print(\"Using features:\", features)\n",
    "\n",
    "# 3) prepare X, y\n",
    "if \"drift_injected\" in df.columns:\n",
    "    y = df[\"drift_injected\"].astype(int).values\n",
    "else:\n",
    "    \n",
    "    for lbl in [\"drift_label\", \"drift_flag_rolling\", \"drift_flag\", \"drift\", \"label\"]:\n",
    "        if lbl in df.columns:\n",
    "            y = df[lbl].astype(int).values\n",
    "            print(\"Using alternative label:\", lbl)\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(\"No label column found (drift_injected/drift_label/drift_flag).\")\n",
    "\n",
    "X = df[features].copy().fillna(df[features].median())\n",
    "\n",
    "# standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 4) OOF calibrated probabilities via cross_val_predict\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "base_clf = LogisticRegression(class_weight='balanced', C=0.5, max_iter=2000, random_state=SEED)\n",
    "\n",
    "calibrator = CalibratedClassifierCV(base_clf, cv=skf, method='sigmoid')\n",
    "print(\"Computing OOF calibrated probabilities (this may take a moment)...\")\n",
    "\n",
    "y_proba_oof = cross_val_predict(calibrator, X_scaled, y, cv=skf, method='predict_proba', n_jobs=1)[:,1]\n",
    "\n",
    "\n",
    "# 5) choose threshold subject to FP cap\n",
    "n_total = len(y)\n",
    "n_neg = int((y==0).sum())\n",
    "max_fp = int(np.floor(FP_RATE_CAP * int((y==0).sum())))\n",
    "\n",
    "\n",
    "print(f\"Total windows: {n_total}  FP cap (<= {FP_RATE_CAP*100:.1f}%): {max_fp}\")\n",
    "\n",
    "# iterate thresholds and pick highest recall (TPR) subject to FP <= max_fp\n",
    "best_thr = None\n",
    "best_recall = -1.0\n",
    "best_prec = 0.0\n",
    "best_stats = None\n",
    "\n",
    "# we examine candidate thresholds from unique probs plus fine grid\n",
    "cands = np.unique(np.concatenate([np.linspace(0,1,1001), y_proba_oof, np.array([0.001,0.005,0.01,0.02,0.05,0.1])]))\n",
    "cands = np.sort(cands)[::-1]  # high to low to favor higher recall early\n",
    "\n",
    "for t in cands:\n",
    "    preds = (y_proba_oof >= t).astype(int)\n",
    "    FP = int(((preds==1) & (y==0)).sum())\n",
    "    TP = int(((preds==1) & (y==1)).sum())\n",
    "    FN = int(((preds==0) & (y==1)).sum())\n",
    "    TN = int(((preds==0) & (y==0)).sum())\n",
    "    recall = TP / (TP + FN + 1e-12)\n",
    "    if FP <= max_fp:\n",
    "        # pick threshold that maximizes recall; if tie, maximize precision\n",
    "        prec = TP / (TP + FP + 1e-12)\n",
    "        if recall > best_recall or (np.isclose(recall, best_recall) and prec > best_prec):\n",
    "            best_recall = recall\n",
    "            best_prec = prec\n",
    "            best_thr = float(t)\n",
    "            best_stats = {\"TP\":TP,\"FP\":FP,\"TN\":TN,\"FN\":FN,\"precision\":prec,\"recall\":recall}\n",
    "\n",
    "if best_thr is None:\n",
    "    # no threshold satisfied FP cap â€” fallback: choose threshold with smallest FP\n",
    "    print(\"No threshold met FP cap. Falling back to threshold that minimizes FP.\")\n",
    "    unique_ts = np.unique(y_proba_oof)\n",
    "    best_thr = float(np.max(unique_ts))  # most conservative\n",
    "    preds = (y_proba_oof >= best_thr).astype(int)\n",
    "    FP = int(((preds==1) & (y==0)).sum())\n",
    "    TP = int(((preds==1) & (y==1)).sum())\n",
    "    FN = int(((preds==0) & (y==1)).sum())\n",
    "    TN = int(((preds==0) & (y==0)).sum())\n",
    "    best_stats = {\"TP\":TP,\"FP\":FP,\"TN\":TN,\"FN\":FN,\"precision\":TP/(TP+FP+1e-12),\"recall\":TP/(TP+FN+1e-12)}\n",
    "\n",
    "print(\"Chosen threshold:\", best_thr)\n",
    "print(\"OOF stats at chosen threshold:\", best_stats)\n",
    "print(\"OOF ROC-AUC:\", roc_auc_score(y, y_proba_oof))\n",
    "print(\"OOF PR-AUC:\", average_precision_score(y, y_proba_oof))\n",
    "\n",
    "# 6) Fit final calibrated classifier on full data \n",
    "final_cal = CalibratedClassifierCV(base_clf, cv=skf, method='sigmoid')\n",
    "final_cal.fit(X_scaled, y)\n",
    "# get final probabilities on full set\n",
    "final_proba = final_cal.predict_proba(X_scaled)[:,1]\n",
    "final_preds = (final_proba >= best_thr).astype(int)\n",
    "\n",
    "# final confusion matrix & detection ids\n",
    "cm = confusion_matrix(y, final_preds)\n",
    "TP = int(cm[1,1]); FP = int(cm[0,1]); TN = int(cm[0,0]); FN = int(cm[1,0])\n",
    "print(\"\\n--- Final (full-data) confusion matrix ---\")\n",
    "print(cm)\n",
    "print(f\"TP={TP} FP={FP} TN={TN} FN={FN}  FP_rate={FP/n_total:.4f}\")\n",
    "\n",
    "detected_windows = df.loc[final_preds==1, \"window_id\"].tolist() if \"window_id\" in df.columns else np.where(final_preds==1)[0].tolist()\n",
    "print(\"Detected window ids (meta_pred==1):\", detected_windows)\n",
    "\n",
    "# 7) Save model, scaler, threshold, and predictions\n",
    "model_artifact = {\"model\": final_cal, \"scaler\": scaler, \"features\": features, \"threshold\": best_thr}\n",
    "with open(os.path.join(OUT_DIR, \"meta_model_fp_constrained.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(model_artifact, f)\n",
    "print(\"Saved model artifact to:\", os.path.join(OUT_DIR, \"meta_model_fp_constrained.pkl\"))\n",
    "\n",
    "df_out = df.copy().reset_index(drop=True)\n",
    "df_out[\"meta_proba_oof\"] = y_proba_oof\n",
    "df_out[\"meta_proba\"] = final_proba\n",
    "df_out[\"meta_pred\"] = final_preds\n",
    "df_out.to_csv(os.path.join(OUT_DIR, \"full_run_meta_predictions_fp_constrained.csv\"), index=False)\n",
    "print(\"Saved predictions to:\", os.path.join(OUT_DIR, \"full_run_meta_predictions_fp_constrained.csv\"))\n",
    "\n",
    "# 8) Detailed report: TP/FP lists and per-window info\n",
    "report = {\n",
    "    \"n_total\": n_total,\n",
    "    \"n_pos\": int(y.sum()),\n",
    "    \"n_neg\": int(n_total - y.sum()),\n",
    "    \"chosen_threshold\": best_thr,\n",
    "    \"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN,\n",
    "    \"FP_rate\": FP / n_total\n",
    "}\n",
    "print(\"\\nReport:\", report)\n",
    "\n",
    "topk = df_out.sort_values(\"meta_proba\", ascending=False).head(30)[[\"window_id\",\"meta_proba\",\"meta_pred\"] + features]\n",
    "print(\"\\nTop-30 windows by meta_proba:\")\n",
    "print(topk.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5bb7a7-40a4-426d-aa4d-76ecc658ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PATCHED: final confusion matrix, metrics, plots, and saves ---\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score,\n",
    "                             recall_score, f1_score, roc_auc_score,\n",
    "                             average_precision_score, roc_curve, precision_recall_curve)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", rc={\"figure.dpi\": 120})\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y, final_preds)\n",
    "TN, FP, FN, TP = cm.ravel() if cm.size == 4 else (0,0,0,0)  # safe unpack\n",
    "accuracy = accuracy_score(y, final_preds)\n",
    "precision = precision_score(y, final_preds, zero_division=0)\n",
    "recall = recall_score(y, final_preds, zero_division=0)\n",
    "f1 = f1_score(y, final_preds, zero_division=0)\n",
    "rocauc = roc_auc_score(y, final_proba)\n",
    "pr_auc = average_precision_score(y, final_proba)\n",
    "\n",
    "print(\"\\n--- Final (full-data) confusion matrix ---\")\n",
    "print(cm)\n",
    "print(f\"TP={TP} FP={FP} TN={TN} FN={FN}  FP_rate={FP/n_total:.4f}\")\n",
    "print(f\"Accuracy={accuracy:.4f}  Precision={precision:.4f}  Recall={recall:.4f}  F1={f1:.4f}\")\n",
    "print(f\"ROC-AUC={rocauc:.4f}  PR-AUC={pr_auc:.4f}\")\n",
    "\n",
    "# detected windows\n",
    "detected_windows = df.loc[final_preds==1, \"window_id\"].tolist() if \"window_id\" in df.columns else np.where(final_preds==1)[0].tolist()\n",
    "print(\"Detected window ids (meta_pred==1):\", detected_windows)\n",
    "\n",
    "# save a small numeric report\n",
    "report = {\n",
    "    \"TP\": int(TP), \"FP\": int(FP), \"TN\": int(TN), \"FN\": int(FN),\n",
    "    \"accuracy\": float(accuracy), \"precision\": float(precision),\n",
    "    \"recall\": float(recall), \"f1\": float(f1),\n",
    "    \"roc_auc\": float(rocauc), \"pr_auc\": float(pr_auc),\n",
    "    \"n_total\": int(n_total), \"n_pos\": int(y.sum()), \"n_neg\": int(n_total - y.sum())\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"meta_metrics_report.json\"), \"w\") as f:\n",
    "    import json\n",
    "    json.dump(report, f, indent=2)\n",
    "print(\"Saved numeric report to:\", os.path.join(OUT_DIR, \"meta_metrics_report.json\"))\n",
    "\n",
    "# --- Plots ---\n",
    "# 1) Confusion matrix heatmap\n",
    "plt.figure(figsize=(5,4))\n",
    "ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                 xticklabels=[\"pred_neg\",\"pred_pos\"], yticklabels=[\"true_neg\",\"true_pos\"])\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix (final preds)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 2) ROC curve\n",
    "fpr, tpr, _ = roc_curve(y, final_proba)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f\"ROC (AUC={rocauc:.3f})\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 3) Precision-Recall curve\n",
    "prec, rec, _ = precision_recall_curve(y, final_proba)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f\"PR (AP={pr_auc:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 4) Calibration / reliability (optional quick Brier-like check)\n",
    "from sklearn.calibration import calibration_curve\n",
    "prob_true, prob_pred = calibration_curve(y, final_proba, n_bins=10)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(prob_pred, prob_true, marker='o', label=\"calibration\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.title(\"Calibration curve (meta-proba)\")\n",
    "plt.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"calibration_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved plots to:\", OUT_DIR)\n",
    "\n",
    "# Save detected lists & top-k\n",
    "pd.DataFrame({\"detected_window_id\": detected_windows}).to_csv(os.path.join(OUT_DIR, \"detected_window_ids.csv\"), index=False)\n",
    "topk = df_out.sort_values(\"meta_proba\", ascending=False).head(50)\n",
    "topk.to_csv(os.path.join(OUT_DIR, \"top50_meta_proba.csv\"), index=False)\n",
    "print(\"Saved detected list and top-k to OUT_DIR.\")\n",
    "# --- end PATCH ---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
